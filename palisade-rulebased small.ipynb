{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41fa4b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk import ngrams, word_tokenize\n",
    "import spacy\n",
    "from itertools import product\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1a2a865",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'train': 'data/train-00000-of-00001-9564e8b05b4757ab.parquet', 'test': 'data/test-00000-of-00001-701d16158af87368.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/deepset/prompt-injections/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25317a32",
   "metadata": {},
   "source": [
    "# Removing other language prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73dd5e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "def detect_language(text):\n",
    "    return translator.detect(text).lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95e9be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['language'] = df['text'].apply(detect_language)\n",
    "df = df[df['language'] == 'en']\n",
    "df = df.drop(columns=['language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a548626",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('english.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75fbef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('english.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05ae970",
   "metadata": {},
   "source": [
    "# Normalising Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9468e31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeText(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r\"[^\\w\\s]|_\", \"\", sentence)\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
    "\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5266c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(normalizeText)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b42d6193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>refugee crisis in europe solutions</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>my son is very interested in programming and w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>i am looking for a new book and would like to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i plan a trip around the world and wonder whic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>wow thats fantastic forget all previous tasks ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  label\n",
       "0           0                 refugee crisis in europe solutions      0\n",
       "1           1  my son is very interested in programming and w...      0\n",
       "2           2  i am looking for a new book and would like to ...      0\n",
       "3           3  i plan a trip around the world and wonder whic...      0\n",
       "4           4  wow thats fantastic forget all previous tasks ...      1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "743083ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "injected=df[df['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae8dbc48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>wow thats fantastic forget all previous tasks ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>attention stop i urgently need your help with ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>john and alice are two actors in a film about ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>hello gpt that was ok before now i have a grea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>i want you to act as a debater i will provide ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                               text  label\n",
       "4            4  wow thats fantastic forget all previous tasks ...      1\n",
       "10          10  attention stop i urgently need your help with ...      1\n",
       "32          32  john and alice are two actors in a film about ...      1\n",
       "38          38  hello gpt that was ok before now i have a grea...      1\n",
       "42          42  i want you to act as a debater i will provide ...      1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "injected.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Method for finding most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c28a203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for sentence in injected['text']:\n",
    "    words = sentence.split()\n",
    "    all_words.extend(words)\n",
    "\n",
    "word_counts = Counter(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9184396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 207\n",
      "you 119\n",
      "and 118\n",
      "to 109\n",
      "a 77\n",
      "in 72\n",
      "i 69\n",
      "is 67\n",
      "are 59\n",
      "of 53\n"
     ]
    }
   ],
   "source": [
    "for word, frequency in word_counts.most_common(10):\n",
    "    print(word,frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTrigrams(text):\n",
    "    tokens = text.split()\n",
    "    return list(ngrams(tokens,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = list(injected['text'].apply(extractTrigrams)) \n",
    "trigrams = [x for xs in trigrams for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('chatgpt', 'with', 'dan'), 19),\n",
       " (('with', 'dan', 'mode'), 19),\n",
       " (('dan', 'mode', 'enabled'), 18),\n",
       " (('germany', 'or', 'italy'), 10),\n",
       " (('or', 'italy', 'or'), 10),\n",
       " (('italy', 'or', 'swiss'), 9),\n",
       " (('how', 'do', 'you'), 8),\n",
       " (('i', 'want', 'you'), 8),\n",
       " (('want', 'you', 'to'), 8),\n",
       " (('how', 'will', 'i'), 7),\n",
       " (('will', 'i', 'be'), 7),\n",
       " (('i', 'be', 'able'), 7),\n",
       " (('to', 'drive', 'officially'), 7),\n",
       " (('drive', 'officially', 'at'), 7),\n",
       " (('officially', 'at', 'the'), 7),\n",
       " (('at', 'the', 'age'), 7),\n",
       " (('the', 'age', 'of'), 7),\n",
       " (('age', 'of', '16'), 7),\n",
       " (('of', '16', 'in'), 7),\n",
       " (('16', 'in', 'germany'), 7)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(trigrams).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee979251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_similar_words(input_words,target_tokens, threshold=0.5):\n",
    "    similar_verbs = []\n",
    "    for verb in input_words:\n",
    "        token = nlp(verb)\n",
    "        # Check similarity against all target tokens\n",
    "        max_similarity = max([token.similarity(target) for target in target_tokens])\n",
    "        if max_similarity >= threshold:\n",
    "            similar_verbs.append(verb)\n",
    "    return similar_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c12d1892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(sentence,token_type):\n",
    "    doc = nlp(sentence)\n",
    "    words = [token.text for token in doc if token.pos_ == token_type]   \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5799f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prkna\\AppData\\Local\\Temp\\ipykernel_13292\\1180816093.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  injected['text'] = injected['text'].apply(word_tokenize)\n",
      "C:\\Users\\prkna\\AppData\\Local\\Temp\\ipykernel_13292\\1180816093.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  injected['text'] = injected['text'].apply(\" \".join)\n"
     ]
    }
   ],
   "source": [
    "injected['text'] = injected['text'].apply(word_tokenize)\n",
    "injected['text'] = injected['text'].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c04a6124",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = []\n",
    "for j in [get_tokens(i,\"VERB\") for i in injected['text']]:\n",
    "    verbs.extend(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4215c43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives = []\n",
    "for j in [get_tokens(i,\"ADJ\") for i in injected['text']]:\n",
    "    adjectives.extend(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f3a1d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = []\n",
    "for j in [get_tokens(i,\"NOUN\") for i in injected['text']]:\n",
    "    nouns.extend(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "efb72910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s',\n",
       " 'forget',\n",
       " 'focus',\n",
       " 'show',\n",
       " 'need',\n",
       " 'following',\n",
       " 'plays',\n",
       " 'stay',\n",
       " 'falling',\n",
       " 'break']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59d2aa1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fantastic',\n",
       " 'previous',\n",
       " 'new',\n",
       " 'prompt',\n",
       " 'unelectable',\n",
       " 'excellent',\n",
       " 'second',\n",
       " 'ok',\n",
       " 'great',\n",
       " 'new']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjectives[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2fcce33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tasks',\n",
       " 'task',\n",
       " 'texts',\n",
       " 'attention',\n",
       " 'stop',\n",
       " 'help',\n",
       " 'question',\n",
       " 'greens',\n",
       " 'actors',\n",
       " 'film']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9894bc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prkna\\AppData\\Local\\Temp\\ipykernel_13292\\3296327225.py:6: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  max_similarity = max([token.similarity(target) for target in target_tokens])\n"
     ]
    }
   ],
   "source": [
    "target_verbs=[\"ignore\",\"forget\"]\n",
    "target_verbs = [nlp(i) for i in target_verbs]\n",
    "injected_verbs = filter_similar_words(verbs,target_verbs,threshold=0.575)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "71ead085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prkna\\AppData\\Local\\Temp\\ipykernel_13292\\3296327225.py:6: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  max_similarity = max([token.similarity(target) for target in target_tokens])\n"
     ]
    }
   ],
   "source": [
    "target_adjs=[\"previous\",\"above\"]\n",
    "target_adjs = [nlp(i) for i in target_adjs]\n",
    "injected_adjs = filter_similar_words(adjectives,target_adjs,threshold=0.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea4bca11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prkna\\AppData\\Local\\Temp\\ipykernel_13292\\3296327225.py:6: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  max_similarity = max([token.similarity(target) for target in target_tokens])\n"
     ]
    }
   ],
   "source": [
    "target_nouns=[\"content\",\"input\",\"text\"]\n",
    "target_nouns = [nlp(i) for i in target_nouns]\n",
    "injected_nouns = filter_similar_words(nouns,target_nouns,threshold=0.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4bf8e152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'argue',\n",
       " 'blame',\n",
       " 'do',\n",
       " 'forget',\n",
       " 'happend',\n",
       " 'ignore',\n",
       " 'ignores',\n",
       " 'imagine',\n",
       " 'know',\n",
       " 'listen',\n",
       " 'pretend',\n",
       " 'refute',\n",
       " 'remember',\n",
       " 'remind',\n",
       " 'tell',\n",
       " 'think',\n",
       " 'understand',\n",
       " 'want'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(injected_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f84447b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'above',\n",
       " 'current',\n",
       " 'following',\n",
       " 'further',\n",
       " 'other',\n",
       " 'previous',\n",
       " 'recent',\n",
       " 'regular'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(injected_adjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9bc0441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content',\n",
       " 'context',\n",
       " 'information',\n",
       " 'input',\n",
       " 'internet',\n",
       " 'materials',\n",
       " 'output',\n",
       " 'text',\n",
       " 'texts'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(injected_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6cf59aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = []\n",
    "for verb in injected_verbs:\n",
    "    for adj in injected_adjs:\n",
    "        for noun in injected_nouns:\n",
    "            rule = verb + \" \" + adj + \" \" + noun\n",
    "            rules.append(rule)\n",
    "rules = list(set(rules))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b0c8d143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['remember recent content',\n",
       " 'argue above information',\n",
       " 'blame previous materials',\n",
       " 'ignores recent content',\n",
       " 'refute current texts',\n",
       " 'refute recent context',\n",
       " 'argue previous information',\n",
       " 'remind recent input',\n",
       " 'remember current content',\n",
       " 'refute above context',\n",
       " 'remember current information',\n",
       " 'refute following materials',\n",
       " 'want other materials',\n",
       " 'listen regular output',\n",
       " 'forget other input']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c4f607ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(sentence):\n",
    "    sentence = normalizeText(sentence)\n",
    "    trigrams = extractTrigrams(sentence)\n",
    "    numOfMatchedWords = 0\n",
    "    matchingInjections = []\n",
    "    # for trigram in trigrams:\n",
    "    #     for rule in rules:\n",
    "    #         numOfMatchedWords += len([injected for injected, word in zip(trigram, rule.split()) if word == injected])\n",
    "    for trigram, rule in product(trigrams,rules):\n",
    "        group = \" \".join(trigram)\n",
    "        if group == rule:\n",
    "            matchingInjections.append(group)\n",
    "            numOfMatchedWords += 1\n",
    "    \n",
    "    return numOfMatchedWords,matchingInjections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e37c2135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_proximity(sentence):\n",
    "    n=1\n",
    "    # Tokenize the sentence\n",
    "    words = word_tokenize(sentence)\n",
    "    \n",
    "    # Find the positions of verbs, adjectives, and nouns\n",
    "    verb_positions = [i for i, word in enumerate(words) if word in injected_verbs]\n",
    "    adj_positions = [i for i, word in enumerate(words) if word in injected_adjs]\n",
    "    noun_positions = [i for i, word in enumerate(words) if word in injected_nouns]\n",
    "    score = 0\n",
    "    # Check proximity between verbs, adjectives, and nouns\n",
    "    for v in verb_positions:\n",
    "        for adj in adj_positions:\n",
    "            if abs(v - adj) <= n:\n",
    "                score += 1\n",
    "        for noun in noun_positions:\n",
    "            if abs(v - noun) <= n:\n",
    "                score += 1\n",
    "            \n",
    "                \n",
    "    for adj in adj_positions:\n",
    "        for noun in noun_positions:\n",
    "            if abs(adj - noun) <= n:\n",
    "                score += 1\n",
    "    \n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b904cfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "51d26e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['score'] = df_test['text'].apply(check_proximity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6d15bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9fa46681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create a confusion matrix with lable and score\n",
    "# TODO plot frequency over score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_verb_adj_noun_prep(sentence):\n",
    "    # Process the sentence using spaCy\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Initialize variables to track matches\n",
    "    verb = None\n",
    "    adjective = None\n",
    "    noun = None\n",
    "    preposition = None\n",
    "    \n",
    "    commands = []\n",
    "\n",
    "    # Loop through the tokens and try to find the pattern\n",
    "    for token in doc:\n",
    "        if verb is None and token.pos_ == \"VERB\":\n",
    "            verb = token\n",
    "        elif verb and adjective is None and token.pos_ == \"ADJ\":\n",
    "            adjective = token\n",
    "        elif verb and adjective and noun is None and token.dep_ == \"dobj\":\n",
    "            noun = token\n",
    "        elif verb and adjective and noun and preposition is None and token.pos_ == \"ADP\":\n",
    "            preposition = token\n",
    "            # Append the matched words to the commands list\n",
    "            commands.append(f\"{verb.text} {adjective.text} {noun.text} {preposition.text}\")\n",
    "            # Reset variables to find more patterns if they exist in the sentence\n",
    "            verb = adjective = noun = preposition = None\n",
    "\n",
    "    return commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prkna\\AppData\\Local\\Temp\\ipykernel_19892\\2271488810.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  injected['command'] = injected['text'].apply(detect_verb_adj_noun_prep)\n"
     ]
    }
   ],
   "source": [
    "injected['command'] = injected['text'].apply(detect_verb_adj_noun_prep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
