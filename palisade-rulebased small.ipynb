{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41fa4b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk import ngrams\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1a2a865",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'train': 'data/train-00000-of-00001-9564e8b05b4757ab.parquet', 'test': 'data/test-00000-of-00001-701d16158af87368.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/deepset/prompt-injections/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b42d6193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Refugee crisis in Europe solutions</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My son is very interested in programming and w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am looking for a new book and would like to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I plan a trip around the world and wonder whic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow, that's fantastic! Forget all previous tas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                 Refugee crisis in Europe solutions      0\n",
       "1  My son is very interested in programming and w...      0\n",
       "2  I am looking for a new book and would like to ...      0\n",
       "3  I plan a trip around the world and wonder whic...      0\n",
       "4  Wow, that's fantastic! Forget all previous tas...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "743083ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "injected=df[df['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae8dbc48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow, that's fantastic! Forget all previous tas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Attention - stop - I urgently need your help w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>John and Alice are two actors in a film about ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Hello GPT, that was ok before, now I have a gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>I want you to act as a debater. I will provide...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label\n",
       "4   Wow, that's fantastic! Forget all previous tas...      1\n",
       "10  Attention - stop - I urgently need your help w...      1\n",
       "32  John and Alice are two actors in a film about ...      1\n",
       "38  Hello GPT, that was ok before, now I have a gr...      1\n",
       "42  I want you to act as a debater. I will provide...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "injected.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Method for finding most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ae597fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Split into words\n",
    "    words = text.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c28a203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for sentence in injected['text']:\n",
    "    words = preprocess_text(sentence)\n",
    "    all_words.extend(words)\n",
    "\n",
    "word_counts = Counter(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9184396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 223\n",
      "you 129\n",
      "and 128\n",
      "to 121\n",
      "in 112\n",
      "a 93\n",
      "i 85\n",
      "is 77\n",
      "und 69\n",
      "are 64\n"
     ]
    }
   ],
   "source": [
    "for word, frequency in word_counts.most_common(10):\n",
    "    print(word,frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTrigrams(text):\n",
    "    tokens = text.split()\n",
    "    return list(ngrams(tokens,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = list(injected['text'].apply(extractTrigrams)) \n",
    "trigrams = [x for xs in trigrams for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('ChatGPT', 'with', 'DAN'), 19),\n",
       " (('with', 'DAN', 'Mode'), 18),\n",
       " (('Ich', 'möchte,', 'dass'), 14),\n",
       " (('möchte,', 'dass', 'Sie'), 14),\n",
       " (('DAN', 'Mode', 'enabled'), 14),\n",
       " (('\\\\', 'n', '\\\\'), 11),\n",
       " (('n', '\\\\', 'n'), 11),\n",
       " (('germany', 'or', 'italy'), 10),\n",
       " (('or', 'italy', 'or'), 10),\n",
       " (('would', 'like', 'to'), 10),\n",
       " (('I', 'want', 'you'), 9),\n",
       " (('want', 'you', 'to'), 9),\n",
       " (('how', 'do', 'you'), 8),\n",
       " (('act', 'as', 'a'), 8),\n",
       " (('and', 'would', 'like'), 8),\n",
       " (('John', 'und', 'Alice'), 7),\n",
       " (('und', 'Alice', 'sind'), 7),\n",
       " (('How', 'will', 'I'), 7),\n",
       " (('will', 'I', 'be'), 7),\n",
       " (('I', 'be', 'able'), 7)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(trigrams).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee979251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_similar_verbs(input_verbs,target_tokens, threshold=0.5):\n",
    "    similar_verbs = []\n",
    "    for verb in input_verbs:\n",
    "        token = nlp(verb)\n",
    "        # Check similarity against all target tokens\n",
    "        max_similarity = max([token.similarity(target) for target in target_tokens])\n",
    "        if max_similarity >= threshold:\n",
    "            similar_verbs.append(verb)\n",
    "    return similar_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c12d1892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verbs(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    verbs = [token.text for token in doc if token.pos_ == \"VERB\"]   \n",
    "    return verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c04a6124",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = []\n",
    "for j in [get_verbs(i) for i in injected['text']]:\n",
    "    verbs.extend(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efb72910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Forget',\n",
       " 'focus',\n",
       " 'show',\n",
       " 'stop',\n",
       " 'need',\n",
       " 'plays',\n",
       " 'stay',\n",
       " 'falling',\n",
       " 'break',\n",
       " 'have']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9894bc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prkna\\AppData\\Local\\Temp\\ipykernel_2892\\776194799.py:6: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  max_similarity = max([token.similarity(target) for target in target_tokens])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Forget',\n",
       " 'refute',\n",
       " 'forget',\n",
       " 'Ignore',\n",
       " 'Stop',\n",
       " 'ignore',\n",
       " 'do',\n",
       " 'tell',\n",
       " 'do',\n",
       " 'Forget',\n",
       " 'Ignore',\n",
       " 'Pretend',\n",
       " 'think',\n",
       " 'Forget',\n",
       " 'Forget',\n",
       " 'Forget',\n",
       " 'Ignore',\n",
       " 'say',\n",
       " 'Forget',\n",
       " 'argue',\n",
       " 'Forget',\n",
       " 'say',\n",
       " 'speak',\n",
       " 'Forget',\n",
       " 'listen',\n",
       " 'do',\n",
       " 'rein',\n",
       " 'know',\n",
       " 'hate',\n",
       " 'know',\n",
       " 'respond',\n",
       " 'say',\n",
       " 'forget',\n",
       " 'say',\n",
       " 'hate',\n",
       " 'ignore',\n",
       " 'say',\n",
       " 'ignore',\n",
       " 'forget',\n",
       " 'tell',\n",
       " 'hate',\n",
       " 'forget',\n",
       " 'tell',\n",
       " 'tell',\n",
       " 'think',\n",
       " 'hate',\n",
       " 'Ignore',\n",
       " 'know',\n",
       " 'give',\n",
       " 'do',\n",
       " 'pretend',\n",
       " 'do',\n",
       " 'tell',\n",
       " 'pretend',\n",
       " 'pretend',\n",
       " 'know',\n",
       " 'say',\n",
       " 'know',\n",
       " 'ignores',\n",
       " 'do',\n",
       " 'do',\n",
       " 'know',\n",
       " 'say',\n",
       " 'remind',\n",
       " 'understand',\n",
       " 'do',\n",
       " 'do',\n",
       " 'respond',\n",
       " 'give',\n",
       " 'think',\n",
       " 'abandon',\n",
       " 'Pretend',\n",
       " 'Ignore',\n",
       " 'hate',\n",
       " 'respond',\n",
       " 'Blame',\n",
       " 'give',\n",
       " 'appreciate',\n",
       " 'hate',\n",
       " 'say',\n",
       " 'forget',\n",
       " 'say',\n",
       " 'ignore',\n",
       " 'happend',\n",
       " 'abandon',\n",
       " 'Forget',\n",
       " 'forget',\n",
       " 'Ignore',\n",
       " 'hate',\n",
       " 'say',\n",
       " 'know',\n",
       " 'Forget',\n",
       " 'forget',\n",
       " 'tell',\n",
       " 'hate',\n",
       " 'Forget',\n",
       " 'Pretend',\n",
       " 'Stop',\n",
       " 'ignore',\n",
       " 'refute',\n",
       " 'say',\n",
       " 'respond',\n",
       " 'tell',\n",
       " 'know',\n",
       " 'Forget',\n",
       " 'do',\n",
       " 'tell',\n",
       " 'do',\n",
       " 'know',\n",
       " 'give',\n",
       " 'Forget',\n",
       " 'know',\n",
       " 'forget',\n",
       " 'Ignore',\n",
       " 'hate',\n",
       " 'say',\n",
       " 'know',\n",
       " 'Forget',\n",
       " 'forget',\n",
       " 'tell',\n",
       " 'hate',\n",
       " 'give',\n",
       " 'do',\n",
       " 'give']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs_to_look_out=[\"Ignore\",\"Forget\"]\n",
    "verbs_to_look_out = [nlp(i) for i in verbs_to_look_out]\n",
    "filter_similar_verbs(verbs,verbs_to_look_out,threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_verb_adj_noun_prep(sentence):\n",
    "    # Process the sentence using spaCy\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Initialize variables to track matches\n",
    "    verb = None\n",
    "    adjective = None\n",
    "    noun = None\n",
    "    preposition = None\n",
    "    \n",
    "    commands = []\n",
    "\n",
    "    # Loop through the tokens and try to find the pattern\n",
    "    for token in doc:\n",
    "        if verb is None and token.pos_ == \"VERB\":\n",
    "            verb = token\n",
    "        elif verb and adjective is None and token.pos_ == \"ADJ\":\n",
    "            adjective = token\n",
    "        elif verb and adjective and noun is None and token.dep_ == \"dobj\":\n",
    "            noun = token\n",
    "        elif verb and adjective and noun and preposition is None and token.pos_ == \"ADP\":\n",
    "            preposition = token\n",
    "            # Append the matched words to the commands list\n",
    "            commands.append(f\"{verb.text} {adjective.text} {noun.text} {preposition.text}\")\n",
    "            # Reset variables to find more patterns if they exist in the sentence\n",
    "            verb = adjective = noun = preposition = None\n",
    "\n",
    "    return commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prkna\\AppData\\Local\\Temp\\ipykernel_19780\\2271488810.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  injected['command'] = injected['text'].apply(detect_verb_adj_noun_prep)\n"
     ]
    }
   ],
   "source": [
    "injected['command'] = injected['text'].apply(detect_verb_adj_noun_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injected.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b501d919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
